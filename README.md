# Das BÃ¼cherregal

## Requirements

### Conda

Conda is used to manage packages. If you aren't familiar with it, [here's a primer on conda](https://mathigon.org/course/data-science-utilities/conda) (specifically, the Anaconda distribution).

The environment file (`environment.yml`) is located in the project root. The environment file lists all the packages needed by the project. Once you install conda, if you run

```zsh
conda env create -f environment.yml
```

a new conda environment named `das-bue` will be created, and all needed packages will be downloaded.

To run any of the scripts in this project, make sure you run `conda activate das-bue` first, to tell conda to use this environment. You can check which environment is currently active by running `conda info --envs`.

### guttenberg-http

In order to interface with Project Gutenberg, install [guttenberg-http](https://github.com/c-w/gutenberg-http/). You will first need to register for and install [Docker](https://www.docker.com). Once everything is installed, run `docker-compose up --build web` from the guttenberg-http directory. This will begin downloading the entire catalog of Project Gutenberg, and then start a web server at `localhost:8000` which will serve as the API.

**Note:** The initial download of the Gutenberg catalog can take quite a long time.

This is only required for running `identification/identification.py`, which produces `ids.txt`. `ids.txt` is included in version control, so this can be skipped if you do not want to pull updated listings from Project Gutenberg. You can see the last time this was run (and committed) by running `git log -n 1 identification/ids.txt`.

## Structure

### Identification

This section is responsible for the creation of the Project Gutenberg ID list.

`identification/identification.py` queries the above-mentioned guttenberg-http for all German language texts. The `identification/ids.txt` file is created based on these, with the Project Gutenberg IDs for all found texts. IDs listed in `identification/blacklist.txt` are not included in `ids.txt`. Both `ids.txt` and `blacklist.txt` are simple, newline delimited files.

### Collection

This section uses the ID list generated by identification and collects and downloads texts and information referenced by that list.

- `collection/downloader.py` downloads an EPUB file from Gutenberg for each ID listed in `identification/ids.txt`, if one exists. The downloaded file for a given `ID` is saved to `collection/downloads/ID.epub`.
- `collection/split_epubs.py` consumes each downloaded EPUB file and creates new EPUB files for each EPUB Chapter in each full-text file. These &ldquo;chunks&rdquo; are saved, with the `n`th EPUB chapter of a text with `ID` saved to `collection/chunks/ID.n.epub`.
- `collection/textify.py` consumes each downloaded EPUB file and does two things:
	- Creates plaintext versions of each chapter. If a text of `ID` has `n` chapters, those chapters are stored in plaintext files `0.txt`, `1.txt`, &hellip; , `[n-1].txt` in `collection/outputs/ID/`.
	- Compiles text metadata into `collection/output.json`.

### Analysis

This section ingests the downloaded content, and performs analysis.

- `analysis/options.py` defines grammar rules used in analysis.
- `analysis/analysis.py` uses the rules defined in `options.py` and the created files in `collection` to produce:
	- `ruleDefs.json`: a simple ordered JSON object containing pairs of rule titles and descriptions.
	- `ruleGroupDefs.json`: a JSON object mimicking the Python dictionary in `options.py`. However, each rule object includes an `i` attribute, which corresponds to the rules index in the list in `ruleDefs.json`.
	- `rule_counts.json`: Counts of each rule for each section of/full text. At the top level is the text ID, followed by the EPUB chapter number/index or `"whole"` (the sum of all chapters), followed finally by an array of numbers. These indices map to those of the rules in `ruleDefs.json`, and to the `i` attributes of each rule in `ruleGroupDefs.json`.
	- `texts_meta.json`: A file containing metadata for each text and chapter.
	- `rules/`: containing a text file for each ID (as `ID.txt`). Each textfile contains a compressed JSON object that describes matches for each rule for a given text, including up to 7 match excerpts, with up to 100 characters of padding on each. The JSON is compressed by encoding to utf-8, compressing via [zlib](https://docs.python.org/3/library/zlib.html), Base64 encoding, and decoding to ASCII. These texts are decompressed in the interface by the browser, using [pako](http://nodeca.github.io/pako/).
- `analysis/author_info.py` searches Gutenberg for the author of each text, and if the author is found and links can be established to a related Wikipedia page, the URL of the pages (German and English) are recorded in `analysis/wikipedia.json`.

### Interface Layer

This section is the application that interfaces with the database created in the analysis section.

The entire application is a [Vue.js](https://vuejs.org) app, which is located at `interface/`. See `interface/README.md` for more info.

## Makefile

The `Makefile` is used to automate some operations. Learn more about Makefiles [here.](https://makefiletutorial.com)

You can specify a target when you run the Makefile. To run the `foo` target, run `make foo` from the project root.

Here are targets for cleaning the directories and re-running scripts:

- `run-identification`: Removes existing `ids.txt` and runs `identification.py`
- `run-collection`: Removes existing `outputs.json` and `chunks`, `downloads`, and `outputs` directories and runs `downloader.py`, `textify.py`, and `split_epubs.py`.
- `run-analysis`: removes all existing JSON files and the `rules` and `words` directories, and then runs `analysis.py` and `author_info.py`.

Here are targets for moving files from the Python areas of the project to the correct locations in the Vue app source code:
- `all-copy`: Calls the following targets:
	- `analysis-outputs-copy`: Calls the following targets:
		- `json-copy`: Calls the following targets:
			- `texts-meta-copy`: copies `texts_meta.json` to the assets directory
			- `ruleGroupDefs-copy`: copies `ruleGroupDefs.json` to the assets directory
			- `ruleDefs-copy`: copies `ruleDefs.json` to the assets directory
			- `rule-counts-copy`: copies `rule_counts.json` to the assets directory
			- `wikipedia-copy`:copies `Wikipedia.json` to the assets directory
		- `rules-dir-copy`: copies the `analysis/rules` directory to the public directory
	- `epub-chunks-dir-copy`: copies the `collection/chunks` directory to the public directory, and renames it `chunk_epubs`

The default target is `json-copy`, so running `make` is the same as running `make json-copy`.
